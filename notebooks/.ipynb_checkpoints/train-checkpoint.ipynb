{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pickle\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from keras.layers import (AveragePooling1D, Conv1D, Dense, Dropout, Embedding,\n",
    "                          Flatten, GlobalMaxPooling1D, LSTM, MaxPool1D)\n",
    "from keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "os.environ['TF_CPP_MIN_LOG_LEVEL']='2'\n",
    "\n",
    "MAX_SEQUENCE = 1000\n",
    "MAX_WORDS = 2000\n",
    "POLARITY_LABEL = {'negative': 0, 'positive': 1}\n",
    "TRAIN_FILE = '../datasets/umich-sentiment/train.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_dataset(file, rows):\n",
    "\n",
    "    dataset = pd.read_csv(file, nrows=rows)\n",
    "    \n",
    "    X = dataset['text']\n",
    "    Y = dataset['sentiment']\n",
    "\n",
    "    return X.tolist(), Y.tolist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_processed_dataset(file, maxseq=None, maxword=None, rows=None, **kwargs):\n",
    "    from keras.preprocessing.text import Tokenizer\n",
    "    from keras.preprocessing.sequence import pad_sequences\n",
    "    from keras.utils.np_utils import to_categorical\n",
    "\n",
    "    MAX_SEQ = maxseq or MAX_SEQUENCE\n",
    "    MAX_WORDS = maxword or 20000\n",
    "\n",
    "\n",
    "    X_train, Y_train = load_dataset(file, rows)\n",
    "        \n",
    "    tokenizer = Tokenizer(num_words=MAX_WORDS)\n",
    "    tokenizer.fit_on_texts(X_train)\n",
    "    \n",
    "    word_index = tokenizer.word_index\n",
    "    sequences = tokenizer.texts_to_sequences(X_train)\n",
    "    \n",
    "    X1_train = pad_sequences(sequences, maxlen=MAX_SEQ)\n",
    "\n",
    "    Y1_train = to_categorical(np.asarray(Y_train), 2)\n",
    "\n",
    "    del X_train\n",
    "    del Y_train\n",
    "    \n",
    "    return X1_train, Y1_train, word_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def load_embeddings():\n",
    "    return pickle.load(open('../glove/glove.twitter.27B.25d.dict.p', 'rb')), 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "metadata": {},
   "outputs": [],
   "source": [
    "def embedding_layer(word_index):\n",
    "    EMBEDDINGS, EMBEDDING_DIM = load_embeddings()\n",
    "    \n",
    "    matrix = np.zeros((MAX_WORDS, EMBEDDING_DIM)) # Embedding Matrix\n",
    "    max_words = min(MAX_WORDS, len(word_index))\n",
    "\n",
    "    for word, i in word_index.items():\n",
    "        if i >=max_words:\n",
    "            continue\n",
    "        vector = EMBEDDINGS.get(word, None)\n",
    "        if vector is not None:\n",
    "            matrix[i]=vector\n",
    "    \n",
    "    layer = Embedding(MAX_WORDS,\n",
    "                    EMBEDDING_DIM,\n",
    "                    weights=[matrix],\n",
    "                    input_length=MAX_SEQUENCE,\n",
    "                    trainable=False\n",
    "                    )\n",
    "    \n",
    "    return layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def gen_model(sequences):\n",
    "    model = Sequential([\n",
    "    sequences, # sequences: Embedding Sequences\n",
    "    Conv1D(256, 5, activation='relu'),\n",
    "    AveragePooling1D(pool_size=5),\n",
    "    Conv1D(128, 5, activation='relu'),\n",
    "    AveragePooling1D(pool_size=5),\n",
    "    Conv1D(64, 5, activation='relu'),\n",
    "    MaxPool1D(pool_size=5),\n",
    "    GlobalMaxPooling1D(),\n",
    "    Dropout(0.3),\n",
    "    Dense(64, activation='relu'),\n",
    "    Dense(len(POLARITY_LABEL), activation='softmax')\n",
    "    ])\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='rmsprop', metrics=['acc'])\n",
    "    \n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 156,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, Y, word_index = load_processed_dataset(TRAIN_FILE, maxword=MAX_WORDS, tokenize=True)\n",
    "\n",
    "sequences = embedding_layer(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 6377 samples, validate on 709 samples\n",
      "Epoch 1/20\n"
     ]
    }
   ],
   "source": [
    "model = gen_model(sequences)\n",
    "\n",
    "# print(sequences.get_weights()[0][87])\n",
    "model.fit(X, Y, validation_split=0.1, batch_size=256, epochs=20)\n",
    "\n",
    "model_json = model.to_json()\n",
    "with open(\"../model/model.json\", \"w\") as json_file:\n",
    "    json_file.write(model_json)\n",
    "    \n",
    "model.save_weights(\"../model/weights.h5\")\n",
    "model.save(\"../model/sentiment.h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
